
rook-ceph-cluster:
  configOverride: |
    [global]
    osd_pool_default_size = 1
    mon_warn_on_pool_no_redundancy = false
    bdev_flock_retry = 20
    bluefs_buffered_io = false

  cephClusterSpec:
    mon:
      # Set the number of mons to be started. Must be an odd number, and is generally recommended to be 3.
      count: 1
      # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
      # Mons should only be allowed on the same node for test environments where data loss is acceptable.
      allowMultiplePerNode: true

    mgr:
      # When higher availability of the mgr is needed, increase the count to 2.
      # In that case, one mgr will be active and one in standby. When Ceph updates which
      # mgr is active, Rook will update the mgr services to match the active mgr.
      count: 1
      allowMultiplePerNode: true
      modules:
        # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
        # are already enabled by other settings in the cluster CR.
        - name: pg_autoscaler
          enabled: true
    dashboard:
      enabled: true
      port: 7000
    crashCollector:
      disable: true
    storage:
      useAllNodes: true
      useAllDevices: true
      #deviceFilter:
    healthCheck:
      daemonHealth:
        mon:
          disabled: false
          interval: 45s
          timeout: 600s
        osd:
          disabled: false
          interval: 60s
        status:
          disabled: false
          interval: 60s
      # Change pod liveness probe, it works for all mon, mgr, and osd pods.
      livenessProbe:
        mon:
          disabled: false
        mgr:
          disabled: false
        osd:
          disabled: false
    disruptionManagement:
      managePodBudgets: true

  cephBlockPools:
    - name: ceph-bucketpool
      # see https://github.com/rook/rook/blob/master/Documentation/ceph-pool-crd.md#spec for available configuration
      spec:
        failureDomain: osd
        replicated:
          size: 1
          requireSafeReplicaSize: false
      storageClass:
        enabled: true
        name: rook-ceph-retain-bucket
        isDefault: false
        reclaimPolicy: Retain
        volumeBindingMode: Immediate
        provisioner: rook-ceph.ceph.rook.io/bucket
        allowVolumeExpansion: true
        parameters:
          # note: objectStoreNamespace and objectStoreName are configured by the chart
          objectStoreName: block-store1
          objectStoreNamespace: rook-ceph
          region: ch-bs