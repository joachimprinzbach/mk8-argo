
rook-ceph-cluster:
  configOverride: |
    [global]
    osd_pool_default_size = 1
    mon_warn_on_pool_no_redundancy = false
    bdev_flock_retry = 20
    bluefs_buffered_io = false

  cephClusterSpec:
    mon:
      # Set the number of mons to be started. Must be an odd number, and is generally recommended to be 3.
      count: 1
      # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
      # Mons should only be allowed on the same node for test environments where data loss is acceptable.
      allowMultiplePerNode: true

    mgr:
      # When higher availability of the mgr is needed, increase the count to 2.
      # In that case, one mgr will be active and one in standby. When Ceph updates which
      # mgr is active, Rook will update the mgr services to match the active mgr.
      count: 1
      allowMultiplePerNode: true
      modules:
        # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
        # are already enabled by other settings in the cluster CR.
        - name: pg_autoscaler
          enabled: true
    dashboard:
      enabled: true
      port: 7000
    crashCollector:
      disable: true
    storage:
      useAllNodes: true
      useAllDevices: true
      #deviceFilter:
    healthCheck:
      daemonHealth:
        mon:
          disabled: false
          interval: 45s
          timeout: 600s
        osd:
          disabled: false
          interval: 60s
        status:
          disabled: false
          interval: 60s
      # Change pod liveness probe, it works for all mon, mgr, and osd pods.
      livenessProbe:
        mon:
          disabled: false
        mgr:
          disabled: false
        osd:
          disabled: false
    disruptionManagement:
      managePodBudgets: true

  cephBlockPools:
    - name: ceph-blockstore
      # see https://github.com/rook/rook/blob/master/Documentation/ceph-pool-crd.md#spec for available configuration
      spec:
        failureDomain: osd
        replicated:
          size: 1
          requireSafeReplicaSize: false
      storageClass:
        enabled: true
        name: rook-ceph-block
        isDefault: false
        reclaimPolicy: Retain
        allowVolumeExpansion: true
        parameters:
          # note: objectStoreNamespace and objectStoreName are configured by the chart
          clusterID: rook-ceph
          pool: replicapool
          imageFormat: "2"
          imageFeatures: layering
          # The secrets contain Ceph admin credentials. These are generated automatically by the operator
          # in the same namespace as the cluster.
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster
          # Specify the filesystem type of the volume. If not specified, csi-provisioner
          # will set default as `ext4`.
          csi.storage.k8s.io/fstype: ext4
          volumeBindingMode: Immediate
          region: ch-bs

  cephFileSystems:          
    - name: ceph-filesystem
      spec:
        metadataPool:
          replicated:
            size: 1
            requireSafeReplicaSize: false
        dataPools:
          - failureDomain: osd
            replicated:
              size: 1
              requireSafeReplicaSize: false
        preserveFilesystemOnDelete: false
        metadataServer:
          activeCount: 1
          activeStandby: true
      storageClass:
        enabled: true
        name: ceph-filesystem
        parameters:
          # The secrets contain Ceph admin credentials.
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
          # Specify the filesystem type of the volume. If not specified, csi-provisioner
          # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
          # in hyperconverged settings where the volume is mounted on the same node as the osds.
          csi.storage.k8s.io/fstype: ext4
        reclaimPolicy: Retain

cephObjectStores:
  - name: ceph-objectstore
    # see https://github.com/rook/rook/blob/master/Documentation/ceph-object-store-crd.md#object-store-settings for available configuration
    spec:
      metadataPool:
        failureDomain: osd
        replicated:
          size: 1
      dataPool:
        failureDomain: osd
        erasureCoded:
          dataChunks: 2
          codingChunks: 1
      preserveFilesystemOnDelete: false
      gateway:
        port: 80
        # securePort: 443
        # sslCertificateRef:
        instances: 1
      healthCheck:
        bucket:
          interval: 60s
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Retain
      # see https://github.com/rook/rook/blob/master/Documentation/ceph-object-bucket-claim.md#storageclass for available configuration
      parameters:
        # note: objectStoreNamespace and objectStoreName are configured by the chart
        region: ch-bs
